---
title: "first steps, in code and vision"
date: "2025-10-28"
description: "a bit of insight into deltacv's history to kick things off"
---

<script>
import eocvsim_old_img from "$lib/assets/blog/welcome/eocvsim-old.png"
import eocvsim_img from "$lib/assets/eocvsim.png"

import grip_img from "$lib/assets/blog/welcome/the-grip-user-interface.png"

import papervision_img from "$lib/assets/papervision.gif"
</script>

# hello there!

This is Sebastian speaking — the main (*and solo!*) developer at deltacv. Since 2020, I’ve been active in the FIRST community after creating [EOCV-Sim](/eocv-sim). During that time, getting physical access to a robot was becoming increasingly difficult due to pandemic restrictions. I wanted to learn how to use OpenCV for our Ultimate Goal robot, but my only real option was the OpenCV **Python** API. <br><br>
And, well, as all FIRST nerds know — our robots are programmed in **Java**! It was technically possible to develop vision algorithms in Python and then translate them to Java later, since the APIs are conceptually similar. But that approach quickly turned the development process into a slow, frustrating loop of *write → test → translate → debug → repeat*.

Want to slightly tweak a color threshold? Rebuild the code.  
Want to test your pipeline with different types of input sources? Figure it out!

Of course, being a bored 14 year old during a global lockdown wasn't easy, and when I saw people having the same struggle trying to make their own *ring detector* pipelines, I had to put my Java skills to practice.

# eocv-sim's inception

The problem? Every change to a pipeline meant deploying code to the robot, waiting for it to run, debugging on real hardware, and repeating the cycle. This made iterative development slow and frustrating, especially when access to a physical robot was limited.

The concept was simple: create a **simulated environment** where you can feed input sources (live camera, webcam, video file, image folder), apply your vision pipeline code, inspect the results in real time, tune your parameters interactively — then once it’s working, copy‑paste the pipeline onto the robot and go. No need to translate from Python to Java, or setting up OpenCV for the desktop.

**EOCV‑Sim** does exactly that: it replicates parts of the EasyOpenCV and FTC SDK vision infrastructure in a desktop GUI, so you don’t need hardware to iterate the exact same code you'd run in your robot.

<figure>
 <img src={eocvsim_old_img} width=500>
 <figcaption>An early version of EOCV-Sim</figcaption>
</figure>

Over time, EOCV‑Sim grew more powerful and polished. One of the standout features was a variable tuner that could magically adjust values in real time—even OpenCV primitives like `Scalar`—without restarting anything. 
On top of that, **workspaces** turned the simulator into a fully standalone app, letting you compile and load pipeline code comprised of Java source files on the fly. 

Suddenly, what once took tedious cycles of edit‑compile‑test could be done almost instantly, making the simulator feel like a living, interactive lab for your vision pipelines.

<figure>
 <img src={eocvsim_img} width=500>
 <figcaption>Thankfully, I didn't take long to learn of the wonders that FlatLaf could do to a Swing UI.</figcaption>
</figure>

# deltacv is born
At its inception, EOCV‑Sim was hosted on my personal GitHub. In hindsight, I made a rookie mistake: I committed the OpenCV native libraries directly into the repository. 
These libraries aren’t exactly lightweight—and supporting three different platforms meant including three separate sets of binaries! 

Unsurprisingly, GitHub wasn’t thrilled, clone times ballooned, and I quickly realized it was time for a fresh start. So I spun up a new repository and moved the project away from my personal account.

By that point, I’d been involved with three different FIRST teams in my city. My rookie year was spent with *Delta Robotics #9351*, the team that first sparked my passion for building and programming robots. 
And that’s where the “delta” in deltacv comes from: Δ for change, or more practically, delta in computer vision. You could say it hints at making a difference in the CV field—but honestly, I mostly just thought it sounded cool and catchy.

Accidentally pushing binary blobs to GitHub may have been one of my best mistakes yet—creating **deltacv** was the push EOCV‑Sim needed to evolve into the catalyst for something even bigger and cooler.

# vision... visualized?

Even after offering the best tooling when it comes to develop OpenCV pipelines in Java, there was still a few major problems left to address:

## The Challenge of Learning OpenCV in Java as a Rookie

- **Memory leaks are easy to cause**: Each `Mat` stores its data outside Java’s "managed memory", so if you’re not careful—even in a simple loop—you’ll quietly eat up memory while the garbage collector sits clueless.
- **The API is hard to follow**: Computer vision concepts can be tricky to grasp at first, and the Java OpenCV API doesn’t make it any easier. It’s not always clear which methods or parameters correspond to the techniques you want to implement, so applying your understanding in code often feels like guesswork.
- **Documentation is scarce**: The official Java OpenCV docs often leave out important details, forcing you to hunt down example code online just to understand how a method really works. This makes learning and experimenting much slower than it should be. Most examples out there are written in Python!
- **Boilerplate overload**: Even for small changes, you have to repeatedly write similar code for managing Mat objects or applying operations, which slows down experimentation.

That indeed was my experience when trying to actually learn OpenCV, even after having developed the early version of EOCV-Sim, I was still clueless on the actual framework of making vision detection magic!

WPILib’s GRIP helped make that learning curve a bit easier to climb. While the Java code it generated wasn’t exactly stellar, it excelled at one thing: 
visually demonstrating core computer vision concepts, showing how each operation connected to the next.

<figure>
 <img src={grip_img} width=500>
 <figcaption>WPILib's GRIP - screenshot taken directly from their docs</figcaption>
</figure>

I’ve never personally been a fan of visual programming tools. Blocks in FTC? Jeez, I’ve seen teams trying to implement field-centric drive or even odometry in Blocks—essentially a tangle of trigonometry that’s not exactly pleasant to compose in that interface.

However, I felt there was something uniquely powerful about implementing vision pipelines in a node-tree format. 
With the way pipelines work by definition, the node flow mirrors the actual processing of the image: each step takes the output of the previous one, transforms it, and passes it along. 
You can see the logic of the pipeline unfold visually, tweak parameters on the fly, and immediately understand how each OpenCV function flows into the next node and affects the result.

GRIP took away a lot of the pain points of working with OpenCV in Java, breaking things down visually and intuitively. 
Instead of wrestling with code quirks, users could focus on actually learning how to build a reliable vision pipeline.

Unfortunately for me, by 2021, GRIP was pretty much considered abandonware, with the latest alpha releases being full of annoying crashes that made 
it impossible to use ([the last commit was made 4 years ago!](https://github.com/WPIRoboticsProjects/GRIP)). 

This is understandable considering the FRC scope of the software, barely any teams would bother using OpenCV directly, with the focus shifting to co-processors such as the plug-n-play Limelight 
or the open-source PhotonVision, both of which feature powerful zero-code web interfaces that would cover any of the basic in-season computer vision needs of most teams.

# moving forward with PaperVision

After GRIP faded into obscurity, the idea of building something like it—but modern, more flexible, and actually stable—stuck in my mind. 
I didn’t just want a visual tool that *showed* how vision worked; I wanted one that could *teach* it, while giving developers full control over their code. 
Something open-source, actively maintained, and designed with the same accessibility that got people to use EOCV-Sim. That spark slowly grew into what would become **PaperVision**.

The [first commit](https://github.com/deltacv/PaperVision/commit/88633c47e9d33acaa8aa26f0470f2a0d3436aed8) of PaperVision was made in October 2021, but 
[v1.0.0](https://github.com/deltacv/PaperVision/releases/tag/v1.0.0) didn't drop until November 2024!—yep, three whole years!

Well, aside from the fact that life and high school happened, the project often stalled due to my own knowledge limitations.
As it turns out, making a fully fledged vision node editor presents all sorts of challenges, which are slow to tackle as a solo dev,
and getting stuck at a single problem for too long would often lower my overall motivation with the project.

To this date, I still wouldn’t have called myself a seasoned developer — especially when it comes to structuring something that resembled a *real* software project. 
So, when PaperVision began, I wanted to get the foundation right. I spent extra time planning out how the core systems would fit together and what would make the project scale gracefully later on. 
I went with **Kotlin** as the main language — partly because I was already comfortable with it, but also because its expressive syntax and advanced features felt like the perfect match for what I had in mind. 

> That decision paid off: the entire internal code-generation API is built on top of **Kotlin DSLs**, and it’s one of the reasons I can easily develop new vision nodes!

<!---
The UI was built with `dear imgui` thanks to [imgui-java](https://github.com/SpaiR/imgui-java), which fortunately 
includes [imnodes](https://github.com/Nelarius/imnodes)—but aside from those, we don't strongly depend in any other external libraries.
The Code Gen API was built from scratch, 
-->

Looking back, it’s kind of funny how everything connects.
EOCV-Sim started as a quick fix for a really specific problem, and now I’m building something that scratches the same itch on a much bigger scale.
PaperVision feels like the tool I always wished existed when I was learning — a mix of visual clarity, and code freedom.


<figure>
 <img src={papervision_img} width=500>
 <figcaption>PaperVision in action</figcaption>
</figure>

If EOCV-Sim was about learning how to build vision tools,  
then PaperVision is about learning how to see with them.

More on that soon.